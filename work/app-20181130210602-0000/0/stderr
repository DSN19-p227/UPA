18/11/30 21:06:53 ERROR Executor: Exception in task 11.0 in stage 2.0 (TID 51)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.collection.mutable.ResizableArray$class.$init$(ResizableArray.scala:32)
	at scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:49)
	at scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:63)
	at org.apache.spark.rdd.SubtractedRDD.org$apache$spark$rdd$SubtractedRDD$$getSeq$1(SubtractedRDD.scala:97)
	at org.apache.spark.rdd.SubtractedRDD$$anonfun$compute$1.apply(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.SubtractedRDD$$anonfun$compute$1.apply(SubtractedRDD.scala:119)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:06:53 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 51,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.collection.mutable.ResizableArray$class.$init$(ResizableArray.scala:32)
	at scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:49)
	at scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:63)
	at org.apache.spark.rdd.SubtractedRDD.org$apache$spark$rdd$SubtractedRDD$$getSeq$1(SubtractedRDD.scala:97)
	at org.apache.spark.rdd.SubtractedRDD$$anonfun$compute$1.apply(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.SubtractedRDD$$anonfun$compute$1.apply(SubtractedRDD.scala:119)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:07 ERROR Executor: Exception in task 15.0 in stage 2.0 (TID 55)
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$5.apply(ShuffleBlockFetcherIterator.scala:390)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$5.apply(ShuffleBlockFetcherIterator.scala:390)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:342)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:327)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:327)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:348)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:59)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
18/11/30 21:07:08 ERROR Executor: Exception in task 3.0 in stage 2.0 (TID 43)
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$5.apply(ShuffleBlockFetcherIterator.scala:390)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$5.apply(ShuffleBlockFetcherIterator.scala:390)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:342)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:327)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:327)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:348)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:59)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
18/11/30 21:07:08 ERROR ShuffleBlockFetcherIterator: Failed to create input stream from local block
java.io.IOException: Error in opening FileSegmentManagedBuffer{file=/tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/15/shuffle_0_1_0.data, offset=21399920, length=3200661}
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:113)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:375)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:59)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/15/shuffle_0_1_0.data (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.network.buffer.FileSegmentManagedBuffer.createInputStream(FileSegmentManagedBuffer.java:98)
	... 28 more
18/11/30 21:07:08 ERROR Executor: Exception in task 16.0 in stage 2.0 (TID 56)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.HashMap.newNode(HashMap.java:1747)
	at java.util.HashMap.putVal(HashMap.java:642)
	at java.util.HashMap.put(HashMap.java:612)
	at org.apache.spark.rdd.SubtractedRDD.org$apache$spark$rdd$SubtractedRDD$$getSeq$1(SubtractedRDD.scala:98)
	at org.apache.spark.rdd.SubtractedRDD$$anonfun$compute$1.apply(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.SubtractedRDD$$anonfun$compute$1.apply(SubtractedRDD.scala:119)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:08 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 56,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.HashMap.newNode(HashMap.java:1747)
	at java.util.HashMap.putVal(HashMap.java:642)
	at java.util.HashMap.put(HashMap.java:612)
	at org.apache.spark.rdd.SubtractedRDD.org$apache$spark$rdd$SubtractedRDD$$getSeq$1(SubtractedRDD.scala:98)
	at org.apache.spark.rdd.SubtractedRDD$$anonfun$compute$1.apply(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.SubtractedRDD$$anonfun$compute$1.apply(SubtractedRDD.scala:119)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:08 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 55,5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$5.apply(ShuffleBlockFetcherIterator.scala:390)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$5.apply(ShuffleBlockFetcherIterator.scala:390)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:342)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:327)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:327)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:348)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:59)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
18/11/30 21:07:08 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 43,5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$5.apply(ShuffleBlockFetcherIterator.scala:390)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$5.apply(ShuffleBlockFetcherIterator.scala:390)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
	at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:342)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:327)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:327)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:348)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:395)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:59)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:114)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
18/11/30 21:07:08 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/30/shuffle_0_0_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:351)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:295)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:329)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:140)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:113)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:08 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/30/shuffle_0_0_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:351)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:295)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:329)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:140)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:113)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:08 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/30/shuffle_0_0_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:351)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:295)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:329)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:140)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:113)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:08 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/30/shuffle_0_0_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:199)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:351)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:295)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:329)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:140)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45)
	at org.apache.spark.rdd.SubtractedRDD.integrate$1(SubtractedRDD.scala:113)
	at org.apache.spark.rdd.SubtractedRDD.compute(SubtractedRDD.scala:119)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/13/temp_shuffle_f6d9607d-29f2-42cd-bfde-eadc89d6e0b2
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/13/temp_shuffle_f6d9607d-29f2-42cd-bfde-eadc89d6e0b2 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/13/temp_shuffle_f6d9607d-29f2-42cd-bfde-eadc89d6e0b2
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3e/temp_shuffle_81148cdd-5ff0-461d-81a5-74b51dd7df89
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3e/temp_shuffle_81148cdd-5ff0-461d-81a5-74b51dd7df89 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3e/temp_shuffle_81148cdd-5ff0-461d-81a5-74b51dd7df89
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3b/temp_shuffle_152a1109-5515-4912-9b1e-90adb5da1a4c
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3b/temp_shuffle_152a1109-5515-4912-9b1e-90adb5da1a4c (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3b/temp_shuffle_152a1109-5515-4912-9b1e-90adb5da1a4c
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/03/temp_shuffle_f9df6f4f-e752-43c5-96ad-645ee9a47676
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/03/temp_shuffle_f9df6f4f-e752-43c5-96ad-645ee9a47676 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/03/temp_shuffle_f9df6f4f-e752-43c5-96ad-645ee9a47676
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/09/temp_shuffle_24bbe6e5-6e02-42b2-95fc-f6b62603297d
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/09/temp_shuffle_24bbe6e5-6e02-42b2-95fc-f6b62603297d (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/09/temp_shuffle_24bbe6e5-6e02-42b2-95fc-f6b62603297d
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/03/temp_shuffle_7442a15c-d887-4ed6-8a40-efb344456650
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/03/temp_shuffle_7442a15c-d887-4ed6-8a40-efb344456650 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/03/temp_shuffle_7442a15c-d887-4ed6-8a40-efb344456650
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1b/temp_shuffle_a4f42e5b-e21b-46f7-acdc-9d1647475378
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1b/temp_shuffle_a4f42e5b-e21b-46f7-acdc-9d1647475378 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1b/temp_shuffle_a4f42e5b-e21b-46f7-acdc-9d1647475378
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3d/temp_shuffle_bd910841-15c3-47ce-a7da-4628879ea99c
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3d/temp_shuffle_bd910841-15c3-47ce-a7da-4628879ea99c (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3d/temp_shuffle_bd910841-15c3-47ce-a7da-4628879ea99c
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/11/temp_shuffle_a88ffd2d-1b89-46b5-9756-d034a2d7210e
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/11/temp_shuffle_a88ffd2d-1b89-46b5-9756-d034a2d7210e (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/11/temp_shuffle_a88ffd2d-1b89-46b5-9756-d034a2d7210e
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1b/temp_shuffle_80241c3e-67a9-47ed-82d4-72932e19c24b
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1b/temp_shuffle_80241c3e-67a9-47ed-82d4-72932e19c24b (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1b/temp_shuffle_80241c3e-67a9-47ed-82d4-72932e19c24b
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1d/temp_shuffle_9700280f-55ea-43e0-97a9-cc4d979b0aa9
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1d/temp_shuffle_9700280f-55ea-43e0-97a9-cc4d979b0aa9 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1d/temp_shuffle_9700280f-55ea-43e0-97a9-cc4d979b0aa9
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2a/temp_shuffle_955629c6-4ea9-4f20-a281-a68b7193a157
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2a/temp_shuffle_955629c6-4ea9-4f20-a281-a68b7193a157 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2a/temp_shuffle_955629c6-4ea9-4f20-a281-a68b7193a157
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/27/temp_shuffle_a214725f-e5fd-4f12-af8e-95b23976c97e
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/27/temp_shuffle_a214725f-e5fd-4f12-af8e-95b23976c97e (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/27/temp_shuffle_a214725f-e5fd-4f12-af8e-95b23976c97e
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/31/temp_shuffle_cd02fbe3-ddd5-4f36-b5bb-ede7e8231293
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/31/temp_shuffle_cd02fbe3-ddd5-4f36-b5bb-ede7e8231293 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/31/temp_shuffle_cd02fbe3-ddd5-4f36-b5bb-ede7e8231293
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2a/temp_shuffle_6e357caa-f5da-408d-8986-f7e700516589
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2a/temp_shuffle_6e357caa-f5da-408d-8986-f7e700516589 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2a/temp_shuffle_6e357caa-f5da-408d-8986-f7e700516589
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/20/temp_shuffle_2a3a50dd-5a30-420f-a599-06c5805eb1b3
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/20/temp_shuffle_2a3a50dd-5a30-420f-a599-06c5805eb1b3 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/20/temp_shuffle_2a3a50dd-5a30-420f-a599-06c5805eb1b3
18/11/30 21:07:09 ERROR Executor: Exception in task 3.0 in stage 0.1 (TID 64)
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3e/temp_shuffle_81148cdd-5ff0-461d-81a5-74b51dd7df89 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/09/temp_shuffle_5826c074-e96a-49a3-b5ca-51143670c116
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/09/temp_shuffle_5826c074-e96a-49a3-b5ca-51143670c116 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/09/temp_shuffle_5826c074-e96a-49a3-b5ca-51143670c116
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/18/temp_shuffle_b8f8776a-f127-46da-b68e-c6c0cef3d7c7
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/18/temp_shuffle_b8f8776a-f127-46da-b68e-c6c0cef3d7c7 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/18/temp_shuffle_b8f8776a-f127-46da-b68e-c6c0cef3d7c7
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/33/temp_shuffle_4eb6b896-8e79-47e9-9be1-953fdaa08064
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/33/temp_shuffle_4eb6b896-8e79-47e9-9be1-953fdaa08064 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/33/temp_shuffle_4eb6b896-8e79-47e9-9be1-953fdaa08064
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/29/temp_shuffle_26a1381b-8788-4b2b-9d0e-83090d48b775
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/29/temp_shuffle_26a1381b-8788-4b2b-9d0e-83090d48b775 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/29/temp_shuffle_26a1381b-8788-4b2b-9d0e-83090d48b775
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/05/temp_shuffle_1a80b729-d438-457e-b716-2c4ac4327c5a
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/05/temp_shuffle_1a80b729-d438-457e-b716-2c4ac4327c5a (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/05/temp_shuffle_1a80b729-d438-457e-b716-2c4ac4327c5a
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/23/temp_shuffle_249a1f2d-e94f-43b2-9191-fbfae287a66f
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/23/temp_shuffle_249a1f2d-e94f-43b2-9191-fbfae287a66f (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/23/temp_shuffle_249a1f2d-e94f-43b2-9191-fbfae287a66f
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3c/temp_shuffle_8025b6dd-3261-47c1-8f5a-ffa19402e403
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3c/temp_shuffle_8025b6dd-3261-47c1-8f5a-ffa19402e403 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3c/temp_shuffle_8025b6dd-3261-47c1-8f5a-ffa19402e403
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/14/temp_shuffle_a567f973-7bc4-4e7d-b9d1-cd75cef6af8c
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/14/temp_shuffle_a567f973-7bc4-4e7d-b9d1-cd75cef6af8c (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1e/temp_shuffle_6e8cc500-da93-447e-86df-25f4d177721f
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1e/temp_shuffle_6e8cc500-da93-447e-86df-25f4d177721f (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/04/temp_shuffle_5a729c6e-bfed-435c-91f8-f5bf21c8e320
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/04/temp_shuffle_5a729c6e-bfed-435c-91f8-f5bf21c8e320 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/1e/temp_shuffle_6e8cc500-da93-447e-86df-25f4d177721f
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/14/temp_shuffle_a567f973-7bc4-4e7d-b9d1-cd75cef6af8c
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/12/temp_shuffle_0d268b0d-c49c-441b-a2e5-3894cc4c0883
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/12/temp_shuffle_0d268b0d-c49c-441b-a2e5-3894cc4c0883 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/04/temp_shuffle_5a729c6e-bfed-435c-91f8-f5bf21c8e320
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/12/temp_shuffle_0d268b0d-c49c-441b-a2e5-3894cc4c0883
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/20/temp_shuffle_ce70ff07-831b-4528-a0cf-bb4db46b131e
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/20/temp_shuffle_ce70ff07-831b-4528-a0cf-bb4db46b131e (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/20/temp_shuffle_ce70ff07-831b-4528-a0cf-bb4db46b131e
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3b/temp_shuffle_898cbb23-6b71-4c19-b447-4b13f2f3d6c3
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3b/temp_shuffle_898cbb23-6b71-4c19-b447-4b13f2f3d6c3 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/3b/temp_shuffle_898cbb23-6b71-4c19-b447-4b13f2f3d6c3
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/0b/temp_shuffle_a662a525-e72b-461d-9c7c-2fd38260c5f2
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/0b/temp_shuffle_a662a525-e72b-461d-9c7c-2fd38260c5f2 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/0b/temp_shuffle_a662a525-e72b-461d-9c7c-2fd38260c5f2
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/29/temp_shuffle_89e8cd46-6084-4625-b897-cd278bf55a61
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/29/temp_shuffle_89e8cd46-6084-4625-b897-cd278bf55a61 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/29/temp_shuffle_89e8cd46-6084-4625-b897-cd278bf55a61
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2e/temp_shuffle_1bc48a4d-4423-4982-9443-89580abdafdf
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2e/temp_shuffle_1bc48a4d-4423-4982-9443-89580abdafdf (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
18/11/30 21:07:09 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/2e/temp_shuffle_1bc48a4d-4423-4982-9443-89580abdafdf
18/11/30 21:07:09 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/29/temp_shuffle_21b46688-b7b7-4abf-ab5c-388d8d455dc3
java.io.FileNotFoundException: /tmp/spark-d2bb59f5-9f45-4988-b9bf-e0f8d0227fbd/executor-2463833b-9ed6-400d-a84f-2c1b90be11b0/blockmgr-9036643e-9792-415c-81bd-eba8e6479e14/29/temp_shuffle_21b46688-b7b7-4abf-ab5c-388d8d455dc3 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter$$anonfun$revertPartialWritesAndClose$2.apply$mcV$sp(DiskBlockObjectWriter.scala:215)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1346)
	at org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:212)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:237)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
